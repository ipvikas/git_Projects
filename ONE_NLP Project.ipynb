{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word: calculator\n",
      "All possible senses for calculator is:  [Synset('calculator.n.01'), Synset('calculator.n.02')]\n",
      "\n",
      "\t Enter a synsets word: calculator.n.01\n",
      "\t Definition of calculator.n.01 is:  an expert at calculation (or at operating calculating machines)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sense_word = input('Enter a word: ')\n",
    "synsets = wn.synsets(sense_word)\n",
    "print('All possible senses for', sense_word,  'is: ',synsets )\n",
    "\n",
    "defintion_word = input('\\n\\t Enter a synsets word: ')\n",
    "print('\\t Definition of', defintion_word,  'is: ',wn.synset(defintion_word).definition() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sense_word = input('Enter a word: ')\n",
    "\n",
    "# for sense_word in synsets:\n",
    "#     print(\"\\nSense: \", sense_word.name())\n",
    "#     print(\"Synonyms: \" , [lemma.name() for lemma in sense_word.lemmas()])\n",
    "#     print(\"Gloss Definition: \" + sense_word.definition())\n",
    "#     print(\"Example Sentemces: \" + str(sense_word.examples()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word to know about their sentiment scores: profit\n",
      "Printing sentiment scores for all the synsets/sense of the word\n",
      "\n",
      "---------------------------------------------------------------\n",
      "Sense\t \t \tPositive  \tNegative \t Neutral\n",
      "---------------------------------------------------------------\n",
      "net_income.n.01\t\t0.0\t\t0.0\t\t1.0\n",
      "profit.n.02\t\t0.875\t\t0.0\t\t0.125\n",
      "profit.v.01\t\t0.375\t\t0.0\t\t0.625\n",
      "profit.v.02\t\t0.125\t\t0.0\t\t0.875\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "sense_word = input('Enter a word to know about their sentiment scores: ')\n",
    "\n",
    "\n",
    "example_senti_syn = swn.senti_synsets(sense_word)\n",
    "print(\"Printing sentiment scores for all the synsets/sense of the word\\n\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(\"Sense\\t \\t \\tPositive  \\tNegative \\t Neutral\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for senti_synset in example_senti_syn:\n",
    "    print(senti_synset.synset.name() + \"\\t\\t\" + str(senti_synset.pos_score())  + \"\\t\\t\" +  str(senti_synset.neg_score())\n",
    "         + \"\\t\\t\" + str(senti_synset.obj_score()))\n",
    "print(\"---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      "\n",
      "When it was first published in \"The Jingle Book\" in 1899 it read:[1]\n",
      "\n",
      "Betty Botter bought a bit of butter;\n",
      "\n",
      "“But,” she said, “this butter's bitter!\n",
      "\n",
      "If I put it in my batter, It will make my batter bitter.\n",
      "\n",
      "But a bit o’ better butter, Will make my batter better.”\n",
      "\n",
      "Then she bought a bit o’ butter, Better than the bitter butter, Made her bitter batter better.\n",
      "\n",
      "So ’twas better Betty Botter\n",
      "\n",
      "Bought a bit o’ better butter.\n",
      "\n",
      "--------------------\n",
      "'Brisk brave brigadiers brandished broad bright blades, blunderbusses, and bludgeons—balancing them badly.'\n",
      "\n",
      "------------------\n",
      "'If you must cross a course cross cow across a crowded cow crossing, cross the cross coarse cow across the crowded cow crossing carefully.'\n",
      "--------------------\n",
      "I wish to wish the wish you wish to wish, but if you wish the wish the witch wishes, I won't wish the wish you wish to wish.\n",
      "------------------\n",
      "How much wood would a woodchuck chuck if a woodchuck could chuck wood?\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "fhand = open('TongueTwister.txt','r',encoding=\"utf8\").read()\n",
    "# print(fhand.read())\n",
    "\n",
    "print(\"Sentence: \\n\\n\"+fhand) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokenize output: \n",
      "\n",
      "['When', 'it', 'was', 'first', 'published', 'in', '``', 'The', 'Jingle', 'Book', \"''\", 'in', '1899', 'it', 'read', ':', '[', '1', ']', 'Betty', 'Botter', 'bought', 'a', 'bit', 'of', 'butter', ';', '“', 'But', ',', '”', 'she', 'said', ',', '“', 'this', 'butter', \"'s\", 'bitter', '!', 'If', 'I', 'put', 'it', 'in', 'my', 'batter', ',', 'It', 'will', 'make', 'my', 'batter', 'bitter', '.', 'But', 'a', 'bit', 'o', '’', 'better', 'butter', ',', 'Will', 'make', 'my', 'batter', 'better.', '”', 'Then', 'she', 'bought', 'a', 'bit', 'o', '’', 'butter', ',', 'Better', 'than', 'the', 'bitter', 'butter', ',', 'Made', 'her', 'bitter', 'batter', 'better', '.', 'So', '’', 'twas', 'better', 'Betty', 'Botter', 'Bought', 'a', 'bit', 'o', '’', 'better', 'butter', '.', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', \"'Brisk\", 'brave', 'brigadiers', 'brandished', 'broad', 'bright', 'blades', ',', 'blunderbusses', ',', 'and', 'bludgeons—balancing', 'them', 'badly', '.', \"'\", '--', '--', '--', '--', '--', '--', '--', '--', '--', \"'If\", 'you', 'must', 'cross', 'a', 'course', 'cross', 'cow', 'across', 'a', 'crowded', 'cow', 'crossing', ',', 'cross', 'the', 'cross', 'coarse', 'cow', 'across', 'the', 'crowded', 'cow', 'crossing', 'carefully', '.', \"'\", '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'I', 'wish', 'to', 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish', ',', 'but', 'if', 'you', 'wish', 'the', 'wish', 'the', 'witch', 'wishes', ',', 'I', 'wo', \"n't\", 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish', '.', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'How', 'much', 'wood', 'would', 'a', 'woodchuck', 'chuck', 'if', 'a', 'woodchuck', 'could', 'chuck', 'wood', '?']\n"
     ]
    }
   ],
   "source": [
    "print(\"word_tokenize output: \\n\")\n",
    "print(word_tokenize(fhand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split tokenize output\n",
      "\n",
      "['When', 'it', 'was', 'first', 'published', 'in', '\"The', 'Jingle', 'Book\"', 'in', '1899', 'it', 'read:[1]', 'Betty', 'Botter', 'bought', 'a', 'bit', 'of', 'butter;', '“But,”', 'she', 'said,', '“this', \"butter's\", 'bitter!', 'If', 'I', 'put', 'it', 'in', 'my', 'batter,', 'It', 'will', 'make', 'my', 'batter', 'bitter.', 'But', 'a', 'bit', 'o’', 'better', 'butter,', 'Will', 'make', 'my', 'batter', 'better.”', 'Then', 'she', 'bought', 'a', 'bit', 'o’', 'butter,', 'Better', 'than', 'the', 'bitter', 'butter,', 'Made', 'her', 'bitter', 'batter', 'better.', 'So', '’twas', 'better', 'Betty', 'Botter', 'Bought', 'a', 'bit', 'o’', 'better', 'butter.', '--------------------', \"'Brisk\", 'brave', 'brigadiers', 'brandished', 'broad', 'bright', 'blades,', 'blunderbusses,', 'and', 'bludgeons—balancing', 'them', \"badly.'\", '------------------', \"'If\", 'you', 'must', 'cross', 'a', 'course', 'cross', 'cow', 'across', 'a', 'crowded', 'cow', 'crossing,', 'cross', 'the', 'cross', 'coarse', 'cow', 'across', 'the', 'crowded', 'cow', 'crossing', \"carefully.'\", '--------------------', 'I', 'wish', 'to', 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish,', 'but', 'if', 'you', 'wish', 'the', 'wish', 'the', 'witch', 'wishes,', 'I', \"won't\", 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish.', '------------------', 'How', 'much', 'wood', 'would', 'a', 'woodchuck', 'chuck', 'if', 'a', 'woodchuck', 'could', 'chuck', 'wood?']\n"
     ]
    }
   ],
   "source": [
    "print(\"split tokenize output\\n\")\n",
    "print(fhand.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL SENTENCE: \n",
      "\n",
      "When it was first published in \"The Jingle Book\" in 1899 it read:[1]\n",
      "\n",
      "Betty Botter bought a bit of butter;\n",
      "\n",
      "“But,” she said, “this butter's bitter!\n",
      "\n",
      "If I put it in my batter, It will make my batter bitter.\n",
      "\n",
      "But a bit o’ better butter, Will make my batter better.”\n",
      "\n",
      "Then she bought a bit o’ butter, Better than the bitter butter, Made her bitter batter better.\n",
      "\n",
      "So ’twas better Betty Botter\n",
      "\n",
      "Bought a bit o’ better butter.\n",
      "\n",
      "--------------------\n",
      "'Brisk brave brigadiers brandished broad bright blades, blunderbusses, and bludgeons—balancing them badly.'\n",
      "\n",
      "------------------\n",
      "'If you must cross a course cross cow across a crowded cow crossing, cross the cross coarse cow across the crowded cow crossing carefully.'\n",
      "--------------------\n",
      "I wish to wish the wish you wish to wish, but if you wish the wish the witch wishes, I won't wish the wish you wish to wish.\n",
      "------------------\n",
      "How much wood would a woodchuck chuck if a woodchuck could chuck wood?\n",
      "\n",
      "SENTENCE TOKENIZED: \n",
      "['When it was first published in \"The Jingle Book\" in 1899 it read:[1]\\n\\nBetty Botter bought a bit of butter;\\n\\n“But,” she said, “this butter\\'s bitter!', 'If I put it in my batter, It will make my batter bitter.', 'But a bit o’ better butter, Will make my batter better.”\\n\\nThen she bought a bit o’ butter, Better than the bitter butter, Made her bitter batter better.', 'So ’twas better Betty Botter\\n\\nBought a bit o’ better butter.', \"--------------------\\n'Brisk brave brigadiers brandished broad bright blades, blunderbusses, and bludgeons—balancing them badly.'\", \"------------------\\n'If you must cross a course cross cow across a crowded cow crossing, cross the cross coarse cow across the crowded cow crossing carefully.'\", \"--------------------\\nI wish to wish the wish you wish to wish, but if you wish the wish the witch wishes, I won't wish the wish you wish to wish.\", '------------------\\nHow much wood would a woodchuck chuck if a woodchuck could chuck wood?']\n",
      "\n",
      "LINE TOKENIZED:  ['When it was first published in \"The Jingle Book\" in 1899 it read:[1]', 'Betty Botter bought a bit of butter;', \"“But,” she said, “this butter's bitter!\", 'If I put it in my batter, It will make my batter bitter.', 'But a bit o’ better butter, Will make my batter better.”', 'Then she bought a bit o’ butter, Better than the bitter butter, Made her bitter batter better.', 'So ’twas better Betty Botter', 'Bought a bit o’ better butter.', '--------------------', \"'Brisk brave brigadiers brandished broad bright blades, blunderbusses, and bludgeons—balancing them badly.'\", '------------------', \"'If you must cross a course cross cow across a crowded cow crossing, cross the cross coarse cow across the crowded cow crossing carefully.'\", '--------------------', \"I wish to wish the wish you wish to wish, but if you wish the wish the witch wishes, I won't wish the wish you wish to wish.\", '------------------', 'How much wood would a woodchuck chuck if a woodchuck could chuck wood?']\n",
      "\n",
      "WORD TOKENIZED:\n",
      "['When', 'it', 'was', 'first', 'published', 'in', '``', 'The', 'Jingle', 'Book', \"''\", 'in', '1899', 'it', 'read', ':', '[', '1', ']', 'Betty', 'Botter', 'bought', 'a', 'bit', 'of', 'butter', ';', '“', 'But', ',', '”', 'she', 'said', ',', '“', 'this', 'butter', \"'s\", 'bitter', '!']\n",
      "['If', 'I', 'put', 'it', 'in', 'my', 'batter', ',', 'It', 'will', 'make', 'my', 'batter', 'bitter', '.']\n",
      "['But', 'a', 'bit', 'o', '’', 'better', 'butter', ',', 'Will', 'make', 'my', 'batter', 'better.', '”', 'Then', 'she', 'bought', 'a', 'bit', 'o', '’', 'butter', ',', 'Better', 'than', 'the', 'bitter', 'butter', ',', 'Made', 'her', 'bitter', 'batter', 'better', '.']\n",
      "['So', '’', 'twas', 'better', 'Betty', 'Botter', 'Bought', 'a', 'bit', 'o', '’', 'better', 'butter', '.']\n",
      "['--', '--', '--', '--', '--', '--', '--', '--', '--', '--', \"'Brisk\", 'brave', 'brigadiers', 'brandished', 'broad', 'bright', 'blades', ',', 'blunderbusses', ',', 'and', 'bludgeons—balancing', 'them', 'badly', '.', \"'\"]\n",
      "['--', '--', '--', '--', '--', '--', '--', '--', '--', \"'If\", 'you', 'must', 'cross', 'a', 'course', 'cross', 'cow', 'across', 'a', 'crowded', 'cow', 'crossing', ',', 'cross', 'the', 'cross', 'coarse', 'cow', 'across', 'the', 'crowded', 'cow', 'crossing', 'carefully', '.', \"'\"]\n",
      "['--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'I', 'wish', 'to', 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish', ',', 'but', 'if', 'you', 'wish', 'the', 'wish', 'the', 'witch', 'wishes', ',', 'I', 'wo', \"n't\", 'wish', 'the', 'wish', 'you', 'wish', 'to', 'wish', '.']\n",
      "['--', '--', '--', '--', '--', '--', '--', '--', '--', 'How', 'much', 'wood', 'would', 'a', 'woodchuck', 'chuck', 'if', 'a', 'woodchuck', 'could', 'chuck', 'wood', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "fhand = open('TongueTwister.txt','r',encoding=\"utf8\").read()\n",
    "\n",
    "print(\"ACTUAL SENTENCE: \\n\\n\"+fhand) \n",
    "print(\"\\nSENTENCE TOKENIZED: \")\n",
    "print(sent_tokenize(fhand))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nLINE TOKENIZED: \", LineTokenizer().tokenize(fhand))\n",
    "\n",
    "\n",
    "print(\"\\nWORD TOKENIZED:\")\n",
    "for t in sent_tokenize(fhand):\n",
    "    print(word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# text = \"I and You were trying to play Cricket but when She came, we all started watching Moview untill power cut happened. Next morning, we did boating aginast river flow, once it moved down for few seconds\"\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "# a = sent_tokenize(fhand)\n",
    "\n",
    "# b = word_tokenize(fhand)\n",
    "# from nltk import FreqDist\n",
    "# f = FreqDist(b)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "['When', 'first', 'published', 'The', 'Jingle', 'Book', '1899', 'read', '1', 'Betty', 'Botter', 'bought', 'bit', 'butter', 'But', 'said', 'butter', 'bitter', 'If', 'I', 'put', 'batter', 'It', 'make', 'batter', 'bitter', 'But', 'bit', 'better', 'butter', 'Will', 'make', 'batter', 'better.', 'Then', 'bought', 'bit', 'butter', 'Better', 'bitter', 'butter', 'Made', 'bitter', 'batter', 'better', 'So', 'twas', 'better', 'Betty', 'Botter', 'Bought', 'bit', 'better', 'butter', \"'Brisk\", 'brave', 'brigadiers', 'brandished', 'broad', 'bright', 'blades', 'blunderbusses', 'bludgeons—balancing', 'badly', \"'If\", 'must', 'cross', 'course', 'cross', 'cow', 'across', 'crowded', 'cow', 'crossing', 'cross', 'cross', 'coarse', 'cow', 'across', 'crowded', 'cow', 'crossing', 'carefully', 'I', 'wish', 'wish', 'wish', 'wish', 'wish', 'wish', 'wish', 'witch', 'wishes', 'I', 'wo', \"n't\", 'wish', 'wish', 'wish', 'wish', 'How', 'much', 'wood', 'would', 'woodchuck', 'chuck', 'woodchuck', 'could', 'chuck', 'wood']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vikas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# sentence = \"I and You were trying to play Cricket but when She came, we all started watching Moview untill power cut happened. Next morning, we did boating aginast river flow, once it moved down for few seconds\"\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "words = word_tokenize(fhand)\n",
    "stopwordlist = list(stopwords.words('english'))\n",
    "stopwordlist.extend (['(',')','-',':',',',\"'s\",'!',':',\"'\",\"''\",'--','.',':','?',';''[',']','``','o','’','“','”','”','[',';',])\n",
    "\n",
    "clean_tokens = []\n",
    "for word in words:\n",
    "    if word not in stopwordlist:\n",
    "        clean_tokens.append(word)\n",
    "print(len(clean_tokens))\n",
    "\n",
    "print(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi is Bunny a good boy I am confused']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "punctuation = re.compile(r'[-.?!,;:()|0-9]')\n",
    "\n",
    "AI_tokens = ['Hi, is Bunny a good boy?, I am confused!!!']\n",
    "post_puntuation =[]\n",
    "for words in AI_tokens:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if(len(word)>0):\n",
    "        post_puntuation.append(word)\n",
    "post_puntuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When', 'WRB'),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('first', 'RB'),\n",
       " ('published', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('``', '``'),\n",
       " ('The', 'DT'),\n",
       " ('Jingle', 'NNP'),\n",
       " ('Book', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " ('in', 'IN'),\n",
       " ('1899', 'CD'),\n",
       " ('it', 'PRP'),\n",
       " ('read', 'VBD'),\n",
       " (':', ':'),\n",
       " ('[', 'VB'),\n",
       " ('1', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('Betty', 'NNP'),\n",
       " ('Botter', 'NNP'),\n",
       " ('bought', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('bit', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('butter', 'NN'),\n",
       " (';', ':'),\n",
       " ('“', 'CC'),\n",
       " ('But', 'CC'),\n",
       " (',', ','),\n",
       " ('”', 'NNP'),\n",
       " ('she', 'PRP'),\n",
       " ('said', 'VBD'),\n",
       " (',', ','),\n",
       " ('“', 'VBD'),\n",
       " ('this', 'DT'),\n",
       " ('butter', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('bitter', 'NN'),\n",
       " ('!', '.'),\n",
       " ('If', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('put', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('batter', 'NN'),\n",
       " (',', ','),\n",
       " ('It', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('make', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('batter', 'NN'),\n",
       " ('bitter', 'NN'),\n",
       " ('.', '.'),\n",
       " ('But', 'CC'),\n",
       " ('a', 'DT'),\n",
       " ('bit', 'NN'),\n",
       " ('o', 'JJ'),\n",
       " ('’', 'NNP'),\n",
       " ('better', 'RBR'),\n",
       " ('butter', 'NN'),\n",
       " (',', ','),\n",
       " ('Will', 'NNP'),\n",
       " ('make', 'VB'),\n",
       " ('my', 'PRP$'),\n",
       " ('batter', 'NN'),\n",
       " ('better.', 'NN'),\n",
       " ('”', 'NN'),\n",
       " ('Then', 'RB'),\n",
       " ('she', 'PRP'),\n",
       " ('bought', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('bit', 'NN'),\n",
       " ('o', 'JJ'),\n",
       " ('’', 'NNP'),\n",
       " ('butter', 'NN'),\n",
       " (',', ','),\n",
       " ('Better', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('bitter', 'JJ'),\n",
       " ('butter', 'NN'),\n",
       " (',', ','),\n",
       " ('Made', 'VBD'),\n",
       " ('her', 'PRP$'),\n",
       " ('bitter', 'JJ'),\n",
       " ('batter', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " ('.', '.'),\n",
       " ('So', 'RB'),\n",
       " ('’', 'JJ'),\n",
       " ('twas', 'NN'),\n",
       " ('better', 'RBR'),\n",
       " ('Betty', 'NNP'),\n",
       " ('Botter', 'NNP'),\n",
       " ('Bought', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('bit', 'NN'),\n",
       " ('o', 'JJ'),\n",
       " ('’', 'NNP'),\n",
       " ('better', 'JJR'),\n",
       " ('butter', 'NN'),\n",
       " ('.', '.'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " (\"'Brisk\", 'POS'),\n",
       " ('brave', 'VBP'),\n",
       " ('brigadiers', 'NNS'),\n",
       " ('brandished', 'VBD'),\n",
       " ('broad', 'JJ'),\n",
       " ('bright', 'NN'),\n",
       " ('blades', 'NNS'),\n",
       " (',', ','),\n",
       " ('blunderbusses', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('bludgeons—balancing', 'VBG'),\n",
       " ('them', 'PRP'),\n",
       " ('badly', 'RB'),\n",
       " ('.', '.'),\n",
       " (\"'\", \"''\"),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " (\"'If\", 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('cross', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('course', 'NN'),\n",
       " ('cross', 'NN'),\n",
       " ('cow', 'NN'),\n",
       " ('across', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('crowded', 'JJ'),\n",
       " ('cow', 'NN'),\n",
       " ('crossing', 'NN'),\n",
       " (',', ','),\n",
       " ('cross', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('cross', 'NN'),\n",
       " ('coarse', 'NN'),\n",
       " ('cow', 'NN'),\n",
       " ('across', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('crowded', 'JJ'),\n",
       " ('cow', 'NN'),\n",
       " ('crossing', 'VBG'),\n",
       " ('carefully', 'RB'),\n",
       " ('.', '.'),\n",
       " (\"'\", \"''\"),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('I', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('wish', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('wish', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('wish', 'VB'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('if', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('wish', 'JJ'),\n",
       " ('the', 'DT'),\n",
       " ('witch', 'NN'),\n",
       " ('wishes', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('wo', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('wish', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('wish', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('wish', 'VB'),\n",
       " ('.', '.'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('--', ':'),\n",
       " ('How', 'WRB'),\n",
       " ('much', 'JJ'),\n",
       " ('wood', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('a', 'DT'),\n",
       " ('woodchuck', 'JJ'),\n",
       " ('chuck', 'NN'),\n",
       " ('if', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('woodchuck', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('chuck', 'VB'),\n",
       " ('wood', 'NN'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = word_tokenize(fhand)\n",
    "nltk.pos_tag (token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('When', 'NN'), ('it', 'NN'), ('was', 'NNS'), ('first', 'NN'), ('published', 'VBD'), ('in', 'NN'), ('\"The', 'NN'), ('Jingle', 'NN'), ('Book\"', 'NN'), ('in', 'NN'), ('1899', 'CD'), ('it', 'NN'), ('read:[1]', 'NN'), ('Betty', 'NN'), ('Botter', 'NN'), ('bought', 'NN'), ('a', 'AT'), ('bit', 'NN'), ('of', 'PREP'), ('butter;', 'NN'), ('“But,”', 'NN'), ('she', 'NN'), ('said,', 'NN'), ('“this', 'NNS'), (\"butter's\", 'NNS'), ('bitter!', 'NN'), ('If', 'NN'), ('I', 'NN'), ('put', 'NN'), ('it', 'NN'), ('in', 'NN'), ('my', 'NN'), ('batter,', 'NN'), ('It', 'NN'), ('will', 'NN'), ('make', 'NN'), ('my', 'NN'), ('batter', 'NN'), ('bitter.', 'NN'), ('But', 'NN'), ('a', 'AT'), ('bit', 'NN'), ('o’', 'NN'), ('better', 'NN'), ('butter,', 'NN'), ('Will', 'NN'), ('make', 'NN'), ('my', 'NN'), ('batter', 'NN'), ('better.”', 'NN'), ('Then', 'NN'), ('she', 'NN'), ('bought', 'NN'), ('a', 'AT'), ('bit', 'NN'), ('o’', 'NN'), ('butter,', 'NN'), ('Better', 'NN'), ('than', 'NN'), ('the', 'AT'), ('bitter', 'NN'), ('butter,', 'NN'), ('Made', 'NN'), ('her', 'NN'), ('bitter', 'NN'), ('batter', 'NN'), ('better.', 'NN'), ('So', 'NN'), ('’twas', 'NNS'), ('better', 'NN'), ('Betty', 'NN'), ('Botter', 'NN'), ('Bought', 'NN'), ('a', 'AT'), ('bit', 'NN'), ('o’', 'NN'), ('better', 'NN'), ('butter.', 'NN'), ('--------------------', 'NN'), (\"'Brisk\", 'NN'), ('brave', 'NN'), ('brigadiers', 'NNS'), ('brandished', 'VBD'), ('broad', 'NN'), ('bright', 'NN'), ('blades,', 'NN'), ('blunderbusses,', 'NN'), ('and', 'NN'), ('bludgeons—balancing', 'VBG'), ('them', 'NN'), (\"badly.'\", 'NN'), ('------------------', 'NN'), (\"'If\", 'NN'), ('you', 'NN'), ('must', 'NN'), ('cross', 'NNS'), ('a', 'AT'), ('course', 'NN'), ('cross', 'NNS'), ('cow', 'NN'), ('across', 'NNS'), ('a', 'AT'), ('crowded', 'VBD'), ('cow', 'NN'), ('crossing,', 'NN'), ('cross', 'NNS'), ('the', 'AT'), ('cross', 'NNS'), ('coarse', 'NN'), ('cow', 'NN'), ('across', 'NNS'), ('the', 'AT'), ('crowded', 'VBD'), ('cow', 'NN'), ('crossing', 'VBG'), (\"carefully.'\", 'NN'), ('--------------------', 'NN'), ('I', 'NN'), ('wish', 'NN'), ('to', 'PREP'), ('wish', 'NN'), ('the', 'AT'), ('wish', 'NN'), ('you', 'NN'), ('wish', 'NN'), ('to', 'PREP'), ('wish,', 'NN'), ('but', 'NN'), ('if', 'NN'), ('you', 'NN'), ('wish', 'NN'), ('the', 'AT'), ('wish', 'NN'), ('the', 'AT'), ('witch', 'NN'), ('wishes,', 'NN'), ('I', 'NN'), (\"won't\", 'NN'), ('wish', 'NN'), ('the', 'AT'), ('wish', 'NN'), ('you', 'NN'), ('wish', 'NN'), ('to', 'PREP'), ('wish.', 'NN'), ('------------------', 'NN'), ('How', 'NN'), ('much', 'NN'), ('wood', 'NN'), ('would', 'NN'), ('a', 'AT'), ('woodchuck', 'NN'), ('chuck', 'NN'), ('if', 'NN'), ('a', 'AT'), ('woodchuck', 'NN'), ('could', 'NN'), ('chuck', 'NN'), ('wood?', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# RegexpTagger: MANUALLY defining the word types\n",
    "\n",
    "from nltk.tag.sequential import RegexpTagger\n",
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    "        [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "                (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "                (r'.*able$', 'JJ'),                # adjectives\n",
    "                (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "                (r'.*ly$', 'RB'),                  # adverbs\n",
    "                (r'.*s$', 'NNS'),                  # plural nouns\n",
    "                (r'.*ing$', 'VBG'),                # gerunds\n",
    "                (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "                (r'(from|on|to|into|of)$', 'PREP'),   # prepositions\n",
    "                (r'.*', 'NN')                      # nouns (default)\n",
    "        ])\n",
    "\n",
    "print(regexp_tagger.tag(fhand.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pywsd import disambiguate\n",
    "# disambiguate(fhand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import nltk\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Grocery_and_Gourmet_Food_5.json.gz'\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield json.loads(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 19, 2014</td>\n",
       "      <td>A1QVBUH9E1V6I8</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>Jamshed Mathur</td>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1416355200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 13, 2016</td>\n",
       "      <td>A3GEOILWLK86XM</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>itsjustme</td>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>1476316800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 21, 2015</td>\n",
       "      <td>A32RD6L701BIGP</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>Krystal Clifton</td>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>Strong</td>\n",
       "      <td>1448064000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>08 12, 2015</td>\n",
       "      <td>A2UY1O1FBGKIE6</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>U. Kane</td>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>Great tea</td>\n",
       "      <td>1439337600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>05 28, 2015</td>\n",
       "      <td>A3QHVBQYDV7Z6U</td>\n",
       "      <td>4639725183</td>\n",
       "      <td>The Nana</td>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>This is the tea I remembered!</td>\n",
       "      <td>1432771200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      5.0      True  11 19, 2014  A1QVBUH9E1V6I8  4639725183   \n",
       "1      5.0      True  10 13, 2016  A3GEOILWLK86XM  4639725183   \n",
       "2      5.0      True  11 21, 2015  A32RD6L701BIGP  4639725183   \n",
       "3      5.0      True  08 12, 2015  A2UY1O1FBGKIE6  4639725183   \n",
       "4      5.0      True  05 28, 2015  A3QHVBQYDV7Z6U  4639725183   \n",
       "\n",
       "      reviewerName                                         reviewText  \\\n",
       "0   Jamshed Mathur                                No adverse comment.   \n",
       "1        itsjustme                          Gift for college student.   \n",
       "2  Krystal Clifton  If you like strong tea, this is for you. It mi...   \n",
       "3          U. Kane  Love the tea. The flavor is way better than th...   \n",
       "4         The Nana  I have searched everywhere until I browsed Ama...   \n",
       "\n",
       "                         summary  unixReviewTime vote style image  \n",
       "0                     Five Stars      1416355200  NaN   NaN   NaN  \n",
       "1                 Great product.      1476316800  NaN   NaN   NaN  \n",
       "2                         Strong      1448064000  NaN   NaN   NaN  \n",
       "3                      Great tea      1439337600  NaN   NaN   NaN  \n",
       "4  This is the tea I remembered!      1432771200  NaN   NaN   NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('Grocery_and_Gourmet_Food_5.json.gz')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1143860 entries, 0 to 1143859\n",
      "Data columns (total 12 columns):\n",
      "overall           1143860 non-null float64\n",
      "verified          1143860 non-null bool\n",
      "reviewTime        1143860 non-null object\n",
      "reviewerID        1143860 non-null object\n",
      "asin              1143860 non-null object\n",
      "reviewerName      1143722 non-null object\n",
      "reviewText        1143470 non-null object\n",
      "summary           1143641 non-null object\n",
      "unixReviewTime    1143860 non-null int64\n",
      "vote              158202 non-null object\n",
      "style             592086 non-null object\n",
      "image             9510 non-null object\n",
      "dtypes: bool(1), float64(1), int64(1), object(9)\n",
      "memory usage: 105.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No adverse comment.</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gift for college student.</td>\n",
       "      <td>Great product.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If you like strong tea, this is for you. It mi...</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Love the tea. The flavor is way better than th...</td>\n",
       "      <td>Great tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have searched everywhere until I browsed Ama...</td>\n",
       "      <td>This is the tea I remembered!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tea made with Lipton Yellow Label teabags is m...</td>\n",
       "      <td>Four Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I love this tea!  Okay, I'm not a high falutin...</td>\n",
       "      <td>Love this tea!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Discovered this tea at a local Med. Rest....a ...</td>\n",
       "      <td>Great tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Well I bought this tea after being in Malaysia...</td>\n",
       "      <td>Well I bought this tea after being in Malaysia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We really like this tea.  It is definitely dif...</td>\n",
       "      <td>We really like this tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hard to find in the U.S.A.  Exactly as describ...</td>\n",
       "      <td>good quality.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I make the best brewed iced tea with this yell...</td>\n",
       "      <td>Best for brewed iced tea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I have recently started drinking hot tea again...</td>\n",
       "      <td>Not Bad for iced Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I like pretty much all of Lipton's tea... I ju...</td>\n",
       "      <td>A Great Cuppa...!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I was watching a youtube video about buying te...</td>\n",
       "      <td>Strong for this American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>it was ok, but it didn't taste like the Lipton...</td>\n",
       "      <td>not the same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Great taste use it for cold brew - works great...</td>\n",
       "      <td>Great taste use it for cold brew - works great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Best tea for my single cup coffee maker. I pur...</td>\n",
       "      <td>Best tea for my single cup coffee maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Good tea. Way better than baseline Lipton tea....</td>\n",
       "      <td>Four Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>This tea looks like coffee grounds. Brewed it ...</td>\n",
       "      <td>does not look anything like tea.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>OK tea.</td>\n",
       "      <td>TEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>My second favorite tea!  Drink lots it'll make...</td>\n",
       "      <td>Good Stuff for those who like a strong tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Excellent product, price, shipping.</td>\n",
       "      <td>Excellent product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Half yellow label and half black tea brewed in...</td>\n",
       "      <td>... black tea brewed in a French coffee pot wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>excellent</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Less bitter than the Lipton black tea sold in ...</td>\n",
       "      <td>Less Bitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Good tea flavor. Not the best I've ever had, b...</td>\n",
       "      <td>Good flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>The tea is wonderful. I can't find it in any r...</td>\n",
       "      <td>The tea is wonderful. I can't find it in any r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>I wrote the review for this tea for the other ...</td>\n",
       "      <td>yellow label good for the table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>This is the best flavor tea for iced tea I hav...</td>\n",
       "      <td>Great for Iced Tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143830</th>\n",
       "      <td>Hard to say no to this hard candy.  Great butt...</td>\n",
       "      <td>Butterscotch is always a hit!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143831</th>\n",
       "      <td>Don't like the texture</td>\n",
       "      <td>Not worth the money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143832</th>\n",
       "      <td>:)</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143833</th>\n",
       "      <td>We have had it before, and it is fine!</td>\n",
       "      <td>and it is fine!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143834</th>\n",
       "      <td>Excellent</td>\n",
       "      <td>Good flavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143835</th>\n",
       "      <td>Love the taste, just enough pickling flavour. ...</td>\n",
       "      <td>Delicious, a constant in my fridge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143836</th>\n",
       "      <td>Actually I hate this recipe. Way too much vine...</td>\n",
       "      <td>Sweet Pickled Beets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143837</th>\n",
       "      <td>I have used this for frying, the oil doesn't a...</td>\n",
       "      <td>Good for cooking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143838</th>\n",
       "      <td>This is a great broth as a base for all kinds ...</td>\n",
       "      <td>Great soup base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143839</th>\n",
       "      <td>Soy-free, and still made the perfect Ramen bow...</td>\n",
       "      <td>and still made the perfect Ramen bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143840</th>\n",
       "      <td>Tastes great, this is the only non soy miso I'...</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143841</th>\n",
       "      <td>made some soup. not impressed.</td>\n",
       "      <td>Three Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143842</th>\n",
       "      <td>I am very satisfied with this brand of koji ri...</td>\n",
       "      <td>I am very satisfied with this brand of koji rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143843</th>\n",
       "      <td>Unbeatable quality! I purchased different bran...</td>\n",
       "      <td>Unbeatable quality!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143844</th>\n",
       "      <td>Great for making your own Chinese style rice w...</td>\n",
       "      <td>High quality stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143845</th>\n",
       "      <td>Great for making your own miso, though quite p...</td>\n",
       "      <td>Miso-making essential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143846</th>\n",
       "      <td>If you need koji rice for a project at home, w...</td>\n",
       "      <td>If Looking for Kome-Koji aka Koji Rice, Give R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143847</th>\n",
       "      <td>These are great!  And a great price!</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143848</th>\n",
       "      <td>LOVE these seeds.  it is very hard to find rea...</td>\n",
       "      <td>extremely nice seeds!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143849</th>\n",
       "      <td>Great price, great taste.</td>\n",
       "      <td>Five Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143850</th>\n",
       "      <td>I give this item 1 star  solely because the de...</td>\n",
       "      <td>READ ALL !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143851</th>\n",
       "      <td>FATTING</td>\n",
       "      <td>One Star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143852</th>\n",
       "      <td>We like eating pumpkin seeds and adding them t...</td>\n",
       "      <td>We like eating pumpkin seeds and adding them t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143853</th>\n",
       "      <td>I received this candy today,  it arrived in pr...</td>\n",
       "      <td>No Butter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143854</th>\n",
       "      <td>My daughter in law loves this.</td>\n",
       "      <td>Four Stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143855</th>\n",
       "      <td>As a new vegan, it is sometimes difficult to r...</td>\n",
       "      <td>As a new vegan, it is sometimes difficult to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143856</th>\n",
       "      <td>The best thing ever is ordering a product you ...</td>\n",
       "      <td>The best thing ever is ordering a product you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143857</th>\n",
       "      <td>I used to love ranch before I became vegan. It...</td>\n",
       "      <td>Just what the vegan ordered!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143858</th>\n",
       "      <td>I cannot have dairy nor gluten.  This is as cl...</td>\n",
       "      <td>This is as close to Ranch as I will ever be ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143859</th>\n",
       "      <td>Needs improvement to make it taste like real r...</td>\n",
       "      <td>So so</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1143860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviewText  \\\n",
       "0                                      No adverse comment.   \n",
       "1                                Gift for college student.   \n",
       "2        If you like strong tea, this is for you. It mi...   \n",
       "3        Love the tea. The flavor is way better than th...   \n",
       "4        I have searched everywhere until I browsed Ama...   \n",
       "5        Tea made with Lipton Yellow Label teabags is m...   \n",
       "6        I love this tea!  Okay, I'm not a high falutin...   \n",
       "7        Discovered this tea at a local Med. Rest....a ...   \n",
       "8        Well I bought this tea after being in Malaysia...   \n",
       "9        We really like this tea.  It is definitely dif...   \n",
       "10       Hard to find in the U.S.A.  Exactly as describ...   \n",
       "11       I make the best brewed iced tea with this yell...   \n",
       "12       I have recently started drinking hot tea again...   \n",
       "13       I like pretty much all of Lipton's tea... I ju...   \n",
       "14       I was watching a youtube video about buying te...   \n",
       "15       it was ok, but it didn't taste like the Lipton...   \n",
       "16       Great taste use it for cold brew - works great...   \n",
       "17       Best tea for my single cup coffee maker. I pur...   \n",
       "18       Good tea. Way better than baseline Lipton tea....   \n",
       "19       This tea looks like coffee grounds. Brewed it ...   \n",
       "20                                                 OK tea.   \n",
       "21       My second favorite tea!  Drink lots it'll make...   \n",
       "22                     Excellent product, price, shipping.   \n",
       "23       Half yellow label and half black tea brewed in...   \n",
       "24                                               excellent   \n",
       "25       Less bitter than the Lipton black tea sold in ...   \n",
       "26       Good tea flavor. Not the best I've ever had, b...   \n",
       "27       The tea is wonderful. I can't find it in any r...   \n",
       "28       I wrote the review for this tea for the other ...   \n",
       "29       This is the best flavor tea for iced tea I hav...   \n",
       "...                                                    ...   \n",
       "1143830  Hard to say no to this hard candy.  Great butt...   \n",
       "1143831                             Don't like the texture   \n",
       "1143832                                                 :)   \n",
       "1143833             We have had it before, and it is fine!   \n",
       "1143834                                          Excellent   \n",
       "1143835  Love the taste, just enough pickling flavour. ...   \n",
       "1143836  Actually I hate this recipe. Way too much vine...   \n",
       "1143837  I have used this for frying, the oil doesn't a...   \n",
       "1143838  This is a great broth as a base for all kinds ...   \n",
       "1143839  Soy-free, and still made the perfect Ramen bow...   \n",
       "1143840  Tastes great, this is the only non soy miso I'...   \n",
       "1143841                     made some soup. not impressed.   \n",
       "1143842  I am very satisfied with this brand of koji ri...   \n",
       "1143843  Unbeatable quality! I purchased different bran...   \n",
       "1143844  Great for making your own Chinese style rice w...   \n",
       "1143845  Great for making your own miso, though quite p...   \n",
       "1143846  If you need koji rice for a project at home, w...   \n",
       "1143847               These are great!  And a great price!   \n",
       "1143848  LOVE these seeds.  it is very hard to find rea...   \n",
       "1143849                          Great price, great taste.   \n",
       "1143850  I give this item 1 star  solely because the de...   \n",
       "1143851                                            FATTING   \n",
       "1143852  We like eating pumpkin seeds and adding them t...   \n",
       "1143853  I received this candy today,  it arrived in pr...   \n",
       "1143854                     My daughter in law loves this.   \n",
       "1143855  As a new vegan, it is sometimes difficult to r...   \n",
       "1143856  The best thing ever is ordering a product you ...   \n",
       "1143857  I used to love ranch before I became vegan. It...   \n",
       "1143858  I cannot have dairy nor gluten.  This is as cl...   \n",
       "1143859  Needs improvement to make it taste like real r...   \n",
       "\n",
       "                                                   summary  \n",
       "0                                               Five Stars  \n",
       "1                                           Great product.  \n",
       "2                                                   Strong  \n",
       "3                                                Great tea  \n",
       "4                            This is the tea I remembered!  \n",
       "5                                               Four Stars  \n",
       "6                                           Love this tea!  \n",
       "7                                                Great tea  \n",
       "8        Well I bought this tea after being in Malaysia...  \n",
       "9                                  We really like this tea  \n",
       "10                                           good quality.  \n",
       "11                               Best for brewed iced tea.  \n",
       "12                                    Not Bad for iced Tea  \n",
       "13                                       A Great Cuppa...!  \n",
       "14                                Strong for this American  \n",
       "15                                            not the same  \n",
       "16          Great taste use it for cold brew - works great  \n",
       "17                 Best tea for my single cup coffee maker  \n",
       "18                                              Four Stars  \n",
       "19                        does not look anything like tea.  \n",
       "20                                                     TEA  \n",
       "21              Good Stuff for those who like a strong tea  \n",
       "22                                       Excellent product  \n",
       "23       ... black tea brewed in a French coffee pot wo...  \n",
       "24                                              Five Stars  \n",
       "25                                             Less Bitter  \n",
       "26                                             Good flavor  \n",
       "27       The tea is wonderful. I can't find it in any r...  \n",
       "28                         yellow label good for the table  \n",
       "29                                      Great for Iced Tea  \n",
       "...                                                    ...  \n",
       "1143830                      Butterscotch is always a hit!  \n",
       "1143831                                Not worth the money  \n",
       "1143832                                         Five Stars  \n",
       "1143833                                    and it is fine!  \n",
       "1143834                                        Good flavor  \n",
       "1143835                Delicious, a constant in my fridge.  \n",
       "1143836                                Sweet Pickled Beets  \n",
       "1143837                                   Good for cooking  \n",
       "1143838                                    Great soup base  \n",
       "1143839              and still made the perfect Ramen bowl  \n",
       "1143840                                         Five Stars  \n",
       "1143841                                        Three Stars  \n",
       "1143842   I am very satisfied with this brand of koji rice  \n",
       "1143843                                Unbeatable quality!  \n",
       "1143844                                 High quality stuff  \n",
       "1143845                              Miso-making essential  \n",
       "1143846  If Looking for Kome-Koji aka Koji Rice, Give R...  \n",
       "1143847                                         Five Stars  \n",
       "1143848                              extremely nice seeds!  \n",
       "1143849                                         Five Stars  \n",
       "1143850                                         READ ALL !  \n",
       "1143851                                           One Star  \n",
       "1143852  We like eating pumpkin seeds and adding them t...  \n",
       "1143853                                          No Butter  \n",
       "1143854                                         Four Stars  \n",
       "1143855   As a new vegan, it is sometimes difficult to ...  \n",
       "1143856  The best thing ever is ordering a product you ...  \n",
       "1143857                       Just what the vegan ordered!  \n",
       "1143858  This is as close to Ranch as I will ever be ab...  \n",
       "1143859                                              So so  \n",
       "\n",
       "[1143860 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['reviewText','summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "complaints_df = df[['reviewText','summary']]\n",
    "X_train,X_hold = train_test_split(complaints_df,test_size =0.6,random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [word for word in nltk.word_tokenize(text) if (len(word)> 3 and len(word.strip('Xx/'))>2)]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-043ac1612d4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.75\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#, smooth_idf=True, sublinear_tf=False\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "vectorizer_tf = TfidfVectorizer (tokenizer = tokenize, stop_words = 'english',max_df = 0.75, min_df = 50, max_features = 10000, use_idf = True,smooth_idf=True, sublinear_tf=False)\n",
    "tf_vectors = vectorizer_tf.fit_transform(X_train.summary)\n",
    "\n",
    "#, smooth_idf=True, sublinear_tf=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
